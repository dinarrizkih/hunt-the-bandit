{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74c57b9e-f8bc-4a2f-81bd-1937d4ad8c94",
   "metadata": {},
   "outputs": [
    {
     "ename": "TclError",
     "evalue": "invalid command name \".!canvas\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 175\u001b[0m\n\u001b[0;32m    173\u001b[0m prey_state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show:\n\u001b[1;32m--> 175\u001b[0m     \u001b[43mtarget_turtle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprey_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# Train Predator\u001b[39;00m\n\u001b[0;32m    178\u001b[0m x, y \u001b[38;5;241m=\u001b[39m predator_state\n",
      "File \u001b[1;32mC:\\python\\Lib\\turtle.py:1775\u001b[0m, in \u001b[0;36mTNavigator.goto\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Move turtle to an absolute position.\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \n\u001b[0;32m   1747\u001b[0m \u001b[38;5;124;03mAliases: setpos | setposition | goto:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;124;03m(0.00,0.00)\u001b[39;00m\n\u001b[0;32m   1773\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_goto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVec2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1777\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_goto(Vec2D(x, y))\n",
      "File \u001b[1;32mC:\\python\\Lib\\turtle.py:3175\u001b[0m, in \u001b[0;36mRawTurtle._goto\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m   3167\u001b[0m go_modes \u001b[38;5;241m=\u001b[39m ( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drawing,\n\u001b[0;32m   3168\u001b[0m              \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pencolor,\n\u001b[0;32m   3169\u001b[0m              \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pensize,\n\u001b[0;32m   3170\u001b[0m              \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fillpath, \u001b[38;5;28mlist\u001b[39m))\n\u001b[0;32m   3171\u001b[0m screen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\n\u001b[0;32m   3172\u001b[0m undo_entry \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_position, end, go_modes,\n\u001b[0;32m   3173\u001b[0m               (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrentLineItem,\n\u001b[0;32m   3174\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrentLine[:],\n\u001b[1;32m-> 3175\u001b[0m               \u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pointlist\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrentLineItem\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   3176\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[:])\n\u001b[0;32m   3177\u001b[0m               )\n\u001b[0;32m   3178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mundobuffer:\n\u001b[0;32m   3179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mundobuffer\u001b[38;5;241m.\u001b[39mpush(undo_entry)\n",
      "File \u001b[1;32mC:\\python\\Lib\\turtle.py:754\u001b[0m, in \u001b[0;36mTurtleScreenBase._pointlist\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pointlist\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m    747\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"returns list of coordinate-pairs of points of item\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;124;03m    Example (for insiders):\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;124;03m    >>> from turtle import *\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;124;03m    (9.9999999999999982, 0.0)]\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03m    >>> \"\"\"\u001b[39;00m\n\u001b[1;32m--> 754\u001b[0m     cl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    755\u001b[0m     pl \u001b[38;5;241m=\u001b[39m [(cl[i], \u001b[38;5;241m-\u001b[39mcl[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(cl), \u001b[38;5;241m2\u001b[39m)]\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  pl\n",
      "File \u001b[1;32m<string>:1\u001b[0m, in \u001b[0;36mcoords\u001b[1;34m(self, *args, **kw)\u001b[0m\n",
      "File \u001b[1;32mC:\\python\\Lib\\tkinter\\__init__.py:2822\u001b[0m, in \u001b[0;36mCanvas.coords\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2818\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of coordinates for the item given in ARGS.\"\"\"\u001b[39;00m\n\u001b[0;32m   2819\u001b[0m \u001b[38;5;66;03m# XXX Should use _flatten on args\u001b[39;00m\n\u001b[0;32m   2820\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtk\u001b[38;5;241m.\u001b[39mgetdouble(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m   2821\u001b[0m                    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtk\u001b[38;5;241m.\u001b[39msplitlist(\n\u001b[1;32m-> 2822\u001b[0m            \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)]\n",
      "\u001b[1;31mTclError\u001b[0m: invalid command name \".!canvas\""
     ]
    }
   ],
   "source": [
    "import turtle\n",
    "import numpy as np\n",
    "import yaml\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Setup turtle screen\n",
    "screen = turtle.Screen()\n",
    "screen.title(\"Tom & Jerry - Reinforcement Learning\")\n",
    "screen.setup(800, 800)\n",
    "screen.addshape(\"jerry.gif\")\n",
    "screen.addshape(\"tom.gif\")\n",
    "\n",
    "# Setup score display\n",
    "score_turtle = turtle.Turtle()\n",
    "score_turtle.penup()\n",
    "score_turtle.goto(-100, 325)\n",
    "score_turtle.hideturtle()\n",
    "score = 0\n",
    "prey_score = 0\n",
    "score_turtle.write(f\"Tom's Score: {score}  Jerry's score: {prey_score}\", font=(\"Arial\", 10, \"bold\"))\n",
    "\n",
    "# Create Jerry's turtle\n",
    "target_turtle = turtle.Turtle()\n",
    "target_turtle.speed(1000)\n",
    "target_turtle.shape(\"jerry.gif\")\n",
    "target_turtle.penup()\n",
    "target_turtle.goto(-325, -325)\n",
    "\n",
    "target_turtle.pendown()\n",
    "target_turtle.goto(-325, 325)\n",
    "target_turtle.goto(325, 325)\n",
    "target_turtle.goto(325, -325)\n",
    "target_turtle.goto(-325, -325)\n",
    "\n",
    "target_turtle.penup()\n",
    "target_turtle.goto(300, 300)\n",
    "\n",
    "# Create Tom's turtle\n",
    "agent_turtle = turtle.Turtle()\n",
    "agent_turtle.shape(\"tom.gif\")\n",
    "agent_turtle.speed(1000)\n",
    "agent_turtle.penup()\n",
    "agent_turtle.goto(-300, -300)\n",
    "\n",
    "# Create obstacles\n",
    "obstacles = [(0, 0), (50, 0), (100, 0), (0, 50), (0, 100), (-100, 0), (-50, -50), (-100, 250), (250, -200), \n",
    "             (200, -200), (250, -150), (250, -50), (250, 0), (250, 100), (250, 250), (200, 250), (150, 250), \n",
    "             (-200, -250), (-150, -250), (-200, -150), (-200, -100), (-250, -50), (-250, 0), (-250, 50), \n",
    "             (-250, 150), (-250, 250)]\n",
    "\n",
    "for obstacle in obstacles:\n",
    "    obstacle_turtle = turtle.Turtle()\n",
    "    obstacle_turtle.speed(1000)\n",
    "    obstacle_turtle.shape(\"square\")\n",
    "    obstacle_turtle.shapesize(2.5)\n",
    "    obstacle_turtle.color(\"red\")\n",
    "    obstacle_turtle.penup()\n",
    "    obstacle_turtle.goto(obstacle)\n",
    "\n",
    "# Define states\n",
    "states = [(x, y) for x in range(-300, 301, 50) for y in range(-300, 301, 50)]\n",
    "\n",
    "# Define goal states (states without obstacles)\n",
    "goal_states = [state for state in states if state not in obstacles]\n",
    "\n",
    "# Define actions\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "# Initialize Q-tables\n",
    "predator_table = {}\n",
    "prey_table = {}\n",
    "\n",
    "# Load Q-tables from file if available\n",
    "try:\n",
    "    with open('condition.yaml', 'r') as f:\n",
    "        predator_table = yaml.load(f, Loader=yaml.FullLoader)\n",
    "except FileNotFoundError:\n",
    "    predator_table = {}\n",
    "\n",
    "try:\n",
    "    with open('prey.yaml', 'r') as f:\n",
    "        prey_table = yaml.load(f, Loader=yaml.FullLoader)\n",
    "except FileNotFoundError:\n",
    "    prey_table = {}\n",
    "\n",
    "# Define hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "# Q-Learning algorithm\n",
    "predator_state = (-300, -300)  # Start from the initial state\n",
    "prey_state = (300, 300)\n",
    "\n",
    "for s in goal_states:\n",
    "    for g_state in goal_states:\n",
    "        min_duration = 0\n",
    "        min_duration_time = 0\n",
    "\n",
    "        while min_duration_time != 1000:\n",
    "            randomm = True\n",
    "            show = True\n",
    "\n",
    "            if not randomm:\n",
    "                predator_state = s\n",
    "                prey_state = g_state\n",
    "\n",
    "            if show:\n",
    "                agent_turtle.goto(predator_state)\n",
    "                score_turtle.clear()\n",
    "                score_turtle.write(f\"Tom's Score: {score}  Jerry's Score: {prey_score}\", font=(\"Arial\", 10, \"bold\"))\n",
    "                agent_turtle.speed(10)\n",
    "                target_turtle.speed(10)\n",
    "                target_turtle.goto(prey_state)\n",
    "\n",
    "            done = False\n",
    "            start = time.time()\n",
    "            step = 0\n",
    "\n",
    "            while not done:\n",
    "                step += 1\n",
    "\n",
    "                # Train Prey\n",
    "                x, y = predator_state\n",
    "                goal_x, goal_y = prey_state\n",
    "                condition = (x, y, goal_x, goal_y)\n",
    "                if condition not in prey_table:\n",
    "                    prey_table[condition] = {action: 0 for action in actions}\n",
    "\n",
    "                # Choose an action using epsilon-greedy policy\n",
    "                if np.random.uniform() < epsilon:\n",
    "                    action = np.random.choice(actions)  # Explore\n",
    "                else:\n",
    "                    action = max(prey_table[condition], key=prey_table[condition].get)  # Exploit\n",
    "\n",
    "                # Get the next state\n",
    "                if action == 'up':\n",
    "                    next_state = (goal_x, goal_y + 50)\n",
    "                elif action == 'down':\n",
    "                    next_state = (goal_x, goal_y - 50)\n",
    "                elif action == 'left':\n",
    "                    next_state = (goal_x - 50, goal_y)\n",
    "                else:  # 'right'\n",
    "                    next_state = (goal_x + 50, goal_y)\n",
    "\n",
    "                # Get the reward\n",
    "                if next_state not in states:\n",
    "                    next_state = prey_state\n",
    "                    reward = -5  # Penalty for hitting the wall\n",
    "                elif ((next_state[0] - predator_state[0]) ** 2 + (next_state[1] - predator_state[1]) ** 2) <= 5000:\n",
    "                    reward = -10  # Reached the target turtle\n",
    "                elif next_state in obstacles:\n",
    "                    next_state = prey_state\n",
    "                    reward = -5  # Hit the obstacle turtle\n",
    "                else:\n",
    "                    current_distance = ((prey_state[0] - predator_state[0]) ** 2 + (prey_state[1] - predator_state[1]) ** 2) ** (1 / 2)\n",
    "                    next_distance = ((next_state[0] - predator_state[0]) ** 2 + (next_state[1] - predator_state[1]) ** 2) ** (1 / 2)\n",
    "                    reward = (next_distance - current_distance) / 10\n",
    "\n",
    "                x, y = predator_state\n",
    "                goal_x, goal_y = next_state\n",
    "                next_condition = (x, y, goal_x, goal_y)\n",
    "                if next_condition not in prey_table:\n",
    "                    prey_table[next_condition] = {action: 0 for action in actions}\n",
    "\n",
    "                # Update the Q-table\n",
    "                prey_table[condition][action] += alpha * (\n",
    "                        reward + gamma * max(prey_table[next_condition].values()) -\n",
    "                        prey_table[condition][action])\n",
    "\n",
    "                # Update the state and move the agent turtle\n",
    "                prey_state = next_state\n",
    "                if show:\n",
    "                    target_turtle.goto(prey_state)\n",
    "\n",
    "                # Train Predator\n",
    "                x, y = predator_state\n",
    "                goal_x, goal_y = prey_state\n",
    "                condition = (x, y, goal_x, goal_y)\n",
    "                if condition not in predator_table:\n",
    "                    predator_table[condition] = {action: 0 for action in actions}\n",
    "\n",
    "                # Choose an action using epsilon-greedy policy\n",
    "                if np.random.uniform() < epsilon:\n",
    "                    action = np.random.choice(actions)  # Explore\n",
    "                else:\n",
    "                    action = max(predator_table[condition], key=predator_table[condition].get)  # Exploit\n",
    "\n",
    "                # Get the next state\n",
    "                if action == 'up':\n",
    "                    next_state = (x, y + 50)\n",
    "                elif action == 'down':\n",
    "                    next_state = (x, y - 50)\n",
    "                elif action == 'left':\n",
    "                    next_state = (x - 50, y)\n",
    "                else:  # 'right'\n",
    "                    next_state = (x + 50, y)\n",
    "\n",
    "                # Get the reward\n",
    "                if next_state not in states:\n",
    "                    next_state = predator_state\n",
    "                    reward = -5  # Penalty for hitting the wall\n",
    "                elif next_state == prey_state:\n",
    "                    reward = 10  # Reached the target turtle\n",
    "                elif next_state in obstacles:\n",
    "                    next_state = predator_state\n",
    "                    reward = -5  # Hit the obstacle turtle\n",
    "                else:\n",
    "                    reward = -1\n",
    "\n",
    "                x, y = next_state\n",
    "                goal_x, goal_y = prey_state\n",
    "                next_condition = (x, y, goal_x, goal_y)\n",
    "                if next_condition not in predator_table:\n",
    "                    predator_table[next_condition] = {action: 0 for action in actions}\n",
    "\n",
    "                # Update the Q-table\n",
    "                predator_table[condition][action] += alpha * (\n",
    "                        reward + gamma * max(predator_table[next_condition].values()) -\n",
    "                        predator_table[condition][action])\n",
    "\n",
    "                # Update the state and move the agent turtle\n",
    "                predator_state = next_state\n",
    "                if show:\n",
    "                    agent_turtle.goto(predator_state)\n",
    "\n",
    "                if step == 70:\n",
    "                    done = True\n",
    "                    if randomm:\n",
    "                        prey_state = random.choice(goal_states)\n",
    "                    if show:\n",
    "                        prey_score += 1\n",
    "                        target_turtle.hideturtle()\n",
    "                        target_turtle.goto(prey_state)\n",
    "                        target_turtle.showturtle()\n",
    "\n",
    "                if ((prey_state[0] - predator_state[0]) ** 2 + (prey_state[1] - predator_state[1]) ** 2) <= 5000:\n",
    "                    done = True\n",
    "                    if randomm:\n",
    "                        prey_state = random.choice(goal_states)\n",
    "                    if show:\n",
    "                        score += 1\n",
    "                        target_turtle.hideturtle()\n",
    "                        target_turtle.goto(prey_state)\n",
    "                        target_turtle.showturtle()\n",
    "\n",
    "            duration = time.time() - start\n",
    "            if duration > min_duration:\n",
    "                min_duration = duration\n",
    "                min_duration_time = 0\n",
    "            else:\n",
    "                min_duration_time += 1\n",
    "\n",
    "            # Save Q-tables periodically\n",
    "            if min_duration_time % 100 == 0:\n",
    "                with open('condition.yaml', 'w') as f:\n",
    "                    yaml.dump(predator_table, f)\n",
    "                with open('prey.yaml', 'w') as f:\n",
    "                    yaml.dump(prey_table, f)\n",
    "\n",
    "# Save Q-tables at the end\n",
    "with open('condition.yaml', 'w') as f:\n",
    "    yaml.dump(predator_table, f)\n",
    "with open('prey.yaml', 'w') as f:\n",
    "    yaml.dump(prey_table, f)\n",
    "\n",
    "# Keep the screen open until it's closed manually\n",
    "turtle.done()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a598372f-c948-4a21-9ca6-f0f2063f4ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
